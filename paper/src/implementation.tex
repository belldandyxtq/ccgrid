\section{Design and Implementation of Our Cloud-based Burst Buffer System}
\label{sec:implementation}
% To achieve the challenges, we design and implement our cloud-based burst buffer.
% \kento{Will write more later \ldots}

\subsection{The Role of The Cloud-base Burst Buffers}
\begin{figure}[tb]
	\centering
	\includegraphics[width=6cm]{img/architecture_overview-2}
	\caption{The role of the cloud-based burst buffers}
	\label{architecture:overview}
\end{figure}

Figure \ref{architecture:overview} shows an overview of our cloud-based
burst buffer system, which consists of \emph{compute nodes}, \emph{burst
buffers} and a \emph{shared cloud storage}. \emph{Compute nodes} are nodes on which
applications run. \emph{Burst buffers} are provided by the same instances
as \emph{compute nodes}. Unlike \emph{compute nodes}, The instances
of the \emph{burst buffers} provide remote memory buffers for data caching.
When the applications read or write data, the I/O operations are handled via
\emph{burst buffers}.  The \emph{shared cloud storage} is persistent shared
storage such as Amazon S3.
Because the applications read or write all data via the burst buffers, the
the caching operations under burst buffers is agnostic to the applications.
Hence, the applications can benefit from the two-level of storage hierarchy
without knowing the underlying operations.

\subsection{Design of The Cloud-base Burst Buffers}
\begin{figure*}
\centering
\includegraphics[width=16cm]{img/prototype_overview-2}
\caption{Overview of the prototype}
\label{implemetation:overview of prototype}
\end{figure*}

% In order to validate the effectiveness of the cloud-based burst buffers in real
% environments,  evaluate our architecture in a real environment and compare
% with current architecture, we implemented a prototype.
% Figure \ref{implemetation:overview of prototype} shows the design of cloud-based
% burst buffers.
% In our architecture there are several global data such as total number of burst buffer nodes,
% buffered file information need to be shared between all nodes.
% And since in our proposal architecture, numbers of burst buffer nodes can be dynamically determined,
% a global scheduler is needed to decide when and how nodes are added and removed, these decision
% should be made based on global work load and local work load.
% Also a scheduler is needed to handle the operation like re-balance of buffered data when nodes
% addition, and data write back when nodes deletion.
% 
% For above reasons in this implementation, we adopted master-worker model, a master is designed to
% store global information and interactive with client daemon, as well as serve as a scheduler.
% The architecture of our prototype is described as Figure \ref{implemetation:overview of prototype}
% 
% In our architecture, there are three types of nodes:
% \begin{itemize}
% 	\item A master node manages all burst buffer nodes information, maintains a set of buffered file
% 	meta data, handles operation like burst buffer node addition and deletion etc., and interact with
% 	clients.
% 	\item There are several burst buffer nodes response for actually storing data.
% 	\item A daemon runs on each client machine serving for communication with
% 	master and burst buffer nodes, in order to hide architecture detail from user.
% \end{itemize}
% 
% The shared storage system are mounted on both master and burst buffer nodes,
% both of master and burst buffer nodes have the same assess permission to the shared file system.

In order to validate the effectiveness of the cloud-based burst buffer system in
real environments, we implement a prototype of the system.
In our design,
because the number of burst buffer nodes can be dynamically determined, a central management is required to manage when nodes are added and
removed to re-balance the buffered data. In addition, buffered data is
distributed across burst buffer nodes, metadata also needs to be managed by a
control management.For the above reasons, we employ a master-worker model, 
a master is to manage the global information, and handle I/O requests form
compute nodes, and also serve as a scheduler for data placement on burst
buffers.

The architecture of our prototype is illustrated as
Figure~\ref{implemetation:overview of prototype}.
In our architecture, there are three
types of nodes:
\begin{itemize}
	\item Master node: A node managing burst buffer node information for adding and
	removing, maintaining metadata of buffered data, and handling I/O requests
	from compute nodes
	\item Burst buffer nodes: Nodes providing burst buffers using their
	in-memory space
	\item Client daemon: A process running on each compute node, and
	reading or writing data from or to burst buffers based on metadata managed on
	the master node.
\end{itemize}
%The shared storage system are mounted on both master and burst buffer nodes,
% both of master and burst buffer nodes have the same assess permission to the shared file system.

\subsection{File Chunk}
In our implementation, files are usually stored in more than one burst buffer node depends on the files'
size and numbers of burst buffer nodes available.
Files are divided into dynamically determined-size of \emph{chunks} depending
on the sizes of the files, and stored across burst buffer nodes.
% Such one burst buffer node one chunk design simplified our data layout design, and operations in
% burst buffer nodes addition and deletion.
% \kento{I can not understand this. You mean if an application write X bytes
% of data to N of burst buffer nodes, the system divide the data into N of chunks
% with X/N bytes of size, OR totally mutable-size for each chunk ? Please send me
% an email to answer as Q1} \kento{Q2: Given N of burst buffer nodes, can the
% system divide the file into M (< N) ? Please send me an email to answer as Q2} 
Because applications usually generate various sizes of files, data placement
management is important to make use of the memory space.
Since chunk sizes are dynamically determined, our design can avoid
the disadvantage of memory waste in fixed-size chunks, and allocate
memory just needed for buffing data.
%\kento{I can not understand this because of Q1}
% However such one burst buffer node one chunk design and mutable size has some disadvantages.
% First master must maintain the chunk size of each file, this put a burden on master.
% Second, for a large file the chunk size will become very large, cause a coarse-grained write back
% control, and reduce the performance gained by dirty flag.
% \kento{Q3: I can not understand why, because the chunk size is mutable. Please
% send me an email to answer as Q3} Finally, since the chunk size is decided by
% file size, and one burst buffer node store only one chunk, it is difficult to change buffered file size.

Because our cloud-based burst buffer system is served as two-level hierarchical
storage, the system need to manage consistency between files on burst buffers
and shared cloud storage. 
To manage the consistency, we include \emph{dirty flag} in metadata of each
chunk so that modified chunk can be written back to storage.
%This dirty flag can reduce the write back data size and thus reduce the
%Internet congestion.



\subsection{Master Node}
In our master-worker node model, master node is the supervisor of the entire
system. 
The master node is in charge of handling clients' I/O requests, determining file
chunk placements, and executing file operations including file open, read
and write to burst buffer nodes.
Master node also manages burst buffer node information for addition and deletion
of burst buffer nodes to and from the system
Mainly, the master node maintains following two data structures:
\begin{itemize}
  \item Burst buffer nodes information, such as each IP address of burst buffer
  nodes, and available memory sizes.; 
  \item Metadata, such as file pathes, file identifiers, total file size, access
  control, chunk size, dirty flag and a mapping table to locate chunks
  distributed across burst buffer nodes.
\end{itemize}

% Since only master has the global knowledge of file chunk placements, compute
% nodes must connect to the master node to get these information when issuing I/O
% operations. To avoid a performance bottleneck in the master node, we minimize
% involvement of the master node to I/O operations. In our prototype system,
% % However, in such master-worker
% %model, master is easy to become a bottleneck of the whole system, thus we have
% %to minimize the master involvement in file operation, 
% client asks master about the file meta data, including the chunk placement
% information, then client uses these information and connect to burst buffer nodes directly for sending or receiving data.

\subsection{Client Daemon}

\begin{figure}[tb]
	\centering
	\includegraphics[width=8cm]{img/client_daemon-2}
	\caption{Client Daemon}
	\label{implementaion:client_daemon}
\end{figure}

On each compute node, a client daemon runs as an interface to the burst buffer
nodes. The client daemon communicates to the master node and the burst buffer
nodes to send I/O request, then send and receive I/O data for write and read
via a TCP socket.
Figure~\ref{implementaion:client_daemon} illustrates how the client daemon
interacts with the master node, and the burst buffer to complete read and write
operations. All of the I/O operations are completed via the client daemon, the
underlying processes are agnostic to applications. 
% exchanges data with client
% daemon by using \emph{mmap}.
% Users don't know and should not know the information like IP address of master nodes,  client daemon
% is used to hide these information from users.
% By using client daemon, application doesn't need to change their code, just recompile with a
% library.
After receiving open requests from applications, 
the client daemon sends the requests to the master node to receive metadata
including each file chunk size for each burst buffer nodes, and a mapping table
to chunk locations on each burst buffer nodes in which the file is need to be
read or written.
When applications issue read and write requests, the client daemon sends the
request to corresponding burst buffer nodes according to the metadata, 
and transfers file chunks from or to the burst buffer nodes. After
completing the data transfer, the client daemon replay the
acknowledgement.
Data exchange between an application and the client daemon is done through
\emph{mmap}.

%When a user application issue a write request, I/O data will be buffered in that node by client
%daemon, when user close the file, call flush function or I/O data size exceed I/O server buffer
%size, client daemon will try to transfer I/O data to buffer queue in I/O burst buffer, if buffer
%queue in not full, I/O data will be sent to I/O burst buffer via interconnection network and can be
% seen among computing nodes in the same system after that.
%However if buffer queue is full, client daemon will block user application and wait until buffer
%queue is ready to receive new I/O data, causing a low I/O throughput.

%Similarly when user issue a read request, there are two conditions: required file is buffered in
% buffer queue in I/O burst buffer, or file is stored only in storage in another system.
%In the first cases, file can be transferred to computing nodes from buffer queue directly, and can
% achieve a high throughput.
%If data is not in buffer queue, then a read operation described below will be executed, since data
% must be transferred from storage in another system, in this case, throughput will depend on Internet condition and it is hard to achieve a high throughput.

\subsection{File Operations}

\begin{figure}
\centering
\includegraphics[width=8cm]{img/file_operation-2}
\caption{Details of File Operation}
\label{implementation:file operation}
\end{figure}

In this section, we give more details about the file operations in our
implementation of open, close, read and write (Figure
\ref{implementation:file operation}).

When an application opens a file to read, the client daemon first sends the file
path, and the access mode to master nodes, then the master node tries to
identify the file. If the file is in burst buffers, the master node replay to the client
daemon with the metadata identifying the locations of each file
chunk on the burst buffer nodes.
Using the metadata, the client daemon directly access to each chunk of
the files to read the file.
If the file is not in burst buffers, the master node get the file size from
shared cloud storage, and assign burst buffer nodes for the chunk
locations according to file size. After that,
The master node connects to the assigned burst buffer nodes, and sends
requests to allocate memory space for the chunks, and then requests the assigned
burst buffer nodes to read the their chunks of the file from shared cloud storage. After
that, the master node sends metadata identifying the location of the
chunks on the burst buffer nodes to the client daemon, so that the client daemon
can directly read the chunks from each burst buffer nodes.


% As for file opened in only writing since the file need to be create, master get file size from
% client daemon and assign burst buffer nodes after that master send burst buffer nodes IP address as well as the map
% information from chunks to burst buffer nodes to client daemon, then master connects to
% corresponding burst buffer nodes, send allocate chunk request.
% After receiving the map from chunks to burst buffer nodes, client daemon connect to each burst
% buffer nodes in the map, and start I/O with these burst buffer nodes.
When applications open a file to write, the client daemon sends the request to
the master node.
The master node gets the file size from the client daemon, and assigns the burst
buffer nodes. Then, the master nodes sends requests the assigned burst buffer
nodes for memory allocation. After that, the master node send metadata to
the client daemon. The metadata includes information with which the client daemon can identify the
location of the chunks to write on each assigned burst buffer nodes, 
so that the client daemon can directly write the chunks from each burst buffer
nodes. 

Because the master node need to manage the chunk allocate and
de-allocate operations, all the read, write requests are initiated through the
master node. In addition, the master node monitors workloads of each burst
buffer nodes so that the master node can balance the workloads across all the
burst buffer nodes.

% \subsection{Limitation}
% 
% Since this is a prototype of our proposal system, there are several limitations.
% First, in this prototype implementation, both master node and burst buffer nodes use one thread, it means
% there is only one request can be responded at any time.
% this design may cause master easily becoming a bottleneck when the number of client increase.
% Also, since at any time one burst buffer node can transfer data with only one client, when the bandwidth of
% burst buffer node larger than compute node, it will cause the burst buffer node under utilization.
% However, there are some difficulties to change to a multiple threads version, for example, there
% is a dependency between read from shared storage and send to compute nodes request, burst buffer nodes should
% block the reading request from compute nodes until the read from shared storage finished also
% synchronization is needed among read and write operations.
% 
% Second, one burst buffer node one data chunk design simplified master's design, but it also brings several
% limitations. It is difficult to change the file size after buffered it, as well as it reduce the
% benefit gain by using dirty flag.
% 
% Third, in the node addition and deletion, there is no load balance in our prototype.
% Without load balance, after node added into the system, it needs a long time to achieve speed up.
% Also in the node deletion, file write back and re-buffer are required.
% 
% In this paper, our aim is to validate effectiveness of burst buffers in
% cloud.
% Although we have these limitation in prototype, we can still show the effectiveness of burst
% buffers.